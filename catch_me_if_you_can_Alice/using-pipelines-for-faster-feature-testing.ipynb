{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, in this kernel we'll make a baseline submission using sklearn pipelines instead of functions. Some of the advantages:\n",
    "* Less and more readable code\n",
    "* Faster feature engineering\n",
    "* More automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you're working on a local machine, it's always a good practice to write a function for data downloading. It can be as easy as the one shown below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# ALICE_PATH = os.path.join(\"datasets\", \"alice\")\n",
    "\n",
    "# def load_alice_data(alice_path=ALICE_PATH):\n",
    "#     csv_path_train = os.path.join(alice_path, \"train_sessions.csv\")\n",
    "#     csv_path_test = os.path.join(alice_path, \"test_sessions.csv\")\n",
    "#     return pd.read_csv(csv_path_train, index_col='session_id', parse_dates=['time1']), \\\n",
    "#             pd.read_csv(csv_path_test, index_col='session_id', parse_dates=['time1'])\n",
    "\n",
    "# df_train, df_test = load_alice_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, in the kernel, we'll do this manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train_sessions.csv\", index_col='session_id', parse_dates=['time1'])\n",
    "df_test = pd.read_csv(\"../input/test_sessions.csv\", index_col='session_id', parse_dates=['time1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all we should notice, that our data is time dependent. Leaving it shuffled will cause some problems later on during the cross-validation, so let's sort it right away.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_values(by=\"time1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's take a look at the column data types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems like only the first time column is in datetime format. Better convert the rest now, otherwise you'll have a problem with the datetime arithmetic, for example, when calculation session duration as a feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 11):\n",
    "    df_train['time{}'.format(i)] = pd.to_datetime(df_train['time{}'.format(i)])\n",
    "for i in range(2, 11):\n",
    "    df_test['time{}'.format(i)] = pd.to_datetime(df_test['time{}'.format(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'll skip EDA and feature engineering for now – there are plenty of great kernels in this competition, that cover those in depth. Instead I'll try to show you another way of preparing data and training models – using the sklearn ``Pipeline()`` class, instead of good old functions. This approach is based more on object oriented, rather than procedural programming, which in fact reduces the length of your code quite a lot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The key idea here is to write your own classes for data transformation. They must follow a pretty simple template: they should inherit from two base classes – ``BaseEstimator``, ``TransformerMixin`` – and have a ``fit()`` and ``transform()`` methods, which take the dataset (X) as input and have a y value set to None. In pipelines the ``__init__()`` method is not neccessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fill NaN with zero values.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        sites = ['site%s' % i for i in range(1, 11)]\n",
    "        return X[sites].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Prepare a CountVectorizer friendly 2D-list from data.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.values.tolist()\n",
    "        # Convert dataframe rows to strings\n",
    "        return [\" \".join([str(site) for site in row]) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One of the great advantages of pipelines, is that you can use classes for adding features too. You can test new features quite easilly by adding/removing some of them from the class, and then just running the whole pipeline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new attributes to training and test set.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        # intraday features\n",
    "        hour = X['time1'].apply(lambda ts: ts.hour)\n",
    "        morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "        day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "        evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "        \n",
    "        # season features\n",
    "        month = X['time1'].apply(lambda ts: ts.month)\n",
    "        summer = ((month >= 6) & (month <= 8)).astype('int')\n",
    "        \n",
    "        # day of the week features\n",
    "        weekday = X['time1'].apply(lambda ts: ts.weekday()).astype('int')\n",
    "        \n",
    "        # year features\n",
    "        year = X['time1'].apply(lambda ts: ts.year).astype('int')\n",
    "        \n",
    "        X = np.c_[morning.values, day.values, evening.values, summer.values, weekday.values, year.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new features, that should be scaled.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # session time features\n",
    "        times = ['time%s' % i for i in range(1, 11)]\n",
    "        # session duration: take to the power of 1/5 to normalize the distribution\n",
    "        session_duration = (X[times].max(axis=1) - X[times].min(axis=1)).astype('timedelta64[ms]').astype(int) ** 0.2\n",
    "        # number of sites visited in a session\n",
    "        number_of_sites = X[times].isnull().sum(axis=1).apply(lambda x: 10 - x)\n",
    "        # average time spent on one site during a session\n",
    "        time_per_site = (session_duration / number_of_sites) ** 0.2\n",
    "        \n",
    "        X = np.c_[session_duration.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once we've written the classes, we want to combine them into a pipeline. The ``Pipeline()`` class will call ``transform()`` methods on each one of them and return the transformed dataset, which you can pass to another pipeline as many times as you want. Here we have three separate pipelines with different purposes: ``vectorizer_pipeline`` prepares data for ``CountVectorizer()`` class, ``attributes_pipeline`` adds features and ``scaled_attributes_pipeline`` adds scaled features. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pipeline = Pipeline([\n",
    "    (\"preparator\", DataPreparator()),\n",
    "    (\"list_preparator\", ListPreparator()),\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 3), max_features=50000))\n",
    "])\n",
    "\n",
    "attributes_pipeline = Pipeline([\n",
    "    (\"adder\", AttributesAdder())\n",
    "])\n",
    "\n",
    "scaled_attributes_pipeline = Pipeline([\n",
    "    (\"adder\", ScaledAttributesAdder()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally you can combine these pipelines using ``FeatureUnion()`` class, which will merge the resulting datasets from each pipeline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "('vectorizer_pipeline', vectorizer_pipeline),\n",
    "('attributes_pipeline', attributes_pipeline),\n",
    "('scaled_attributes_pipeline', scaled_attributes_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All you need to do at the end, is just call ``fit_transform()`` or ``transform()`` methods on the ``full_pipeline`` and pass them your original datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = full_pipeline.fit_transform(df_train)\n",
    "X_test = full_pipeline.transform(df_test)\n",
    "\n",
    "y_train = df_train[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and submitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As @yorko introduced, we use time-aware cross-validation scheme.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "logit = LogisticRegression(C=1, random_state=42, solver='liblinear')\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=1)\n",
    "\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, train your model on the whole train set and write a function for submitting results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'submit.csv') # 0.95191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, by using the pipeline template you can write fewer lines of code and understand more, what's going on in your data preparation workflow. You can test features much faster. But the greatest thing about pipelines is that you can generalize the data preparation and training process by changing the dataset you use as a the pipeline input. For example, you can use the same pipeline for transforming your train and test set. And even more: you can automate the whole process, when new data becomes available.**\n",
    "\n",
    "**Good luck with Alice!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
